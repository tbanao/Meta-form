import os
import json
import csv
import time
import hashlib
import requests
import re
import random
import pickle
from datetime import datetime

# --- è¨­å®šåƒæ•¸ ---
ACCESS_TOKEN = "EAAH1oqWMsq8BO37rKconweZBXXPFQac7NCNxFbD40RN9SopOp2t3o5xEPQ1zbkrOkKIUoBGPZBXbsxStkXsniH9EE777qANZAGKXNIgMtliLHZBntS2VTp7uDbLhNBZAFwZBShVw8QyOXbYSDFfwqxQCWtzJYbFzktZCJpD3BkyYeaTcOMP2zz0MnZCfppTCYGb8uQZDZD"
PIXEL_ID = "1664521517602334"
CURRENCY = "TWD"
BATCH_SIZE = 50
MAX_RETRIES = 3
VALUE_CHOICES = [19800, 28800, 28000, 34800, 39800, 45800]
BASE_FOLDER = r"C:\Users\tbana\Desktop\å°è©±å¤¾\your_instagram_activity"
LOG_FILE = "upload_log.txt"
EVENT_ID_LOG = "uploaded_event_ids.txt"
OUTPUT_CSV = "æˆäº¤å®¢æˆ¶_only.csv"
PROFILE_MAP_PATH = "user_profile_map.pkl"
MY_ACCOUNT_NAMES = ["thaispa.tw", "ä½ çš„IGå¸³è™Ÿåç¨±"]
KEYWORDS = ["å®Œæˆ", "ä¸‰å¹´å…§", "äº”å®˜", "æ¸…æ™°", "ç¬¬ä¸€å¤©", "é–‹å§‹"]
SKIP_KEYWORDS = ["settings", "account", "devices", "ads", "report", "profile", "connections"]
CITIES = ["taipei", "newtaipei", "taoyuan", "taichung", "tainan", "kaohsiung"]
user_city_map = {}

# --- è¼‰å…¥ä½¿ç”¨è€…å€‹è³‡è³‡æ–™ ---
user_profile_map = {}
if os.path.exists(PROFILE_MAP_PATH):
    with open(PROFILE_MAP_PATH, "rb") as f:
        user_profile_map = pickle.load(f)

# --- å·¥å…·å‡½å¼ ---
def hash_data(text):
    return hashlib.sha256(text.strip().lower().encode("utf-8")).hexdigest() if text else ""

def generate_event_id(username, content, event_time):
    base = f"{username}_{content}_{event_time}"
    return hashlib.md5(base.encode("utf-8")).hexdigest()

def log(message):
    print(message)
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"{time.strftime('%Y-%m-%d %H:%M:%S')} - {message}\n")

def load_uploaded_event_ids():
    if os.path.exists(EVENT_ID_LOG):
        with open(EVENT_ID_LOG, "r", encoding="utf-8") as f:
            return set(line.strip() for line in f if line.strip())
    return set()

def save_uploaded_event_ids(event_ids):
    with open(EVENT_ID_LOG, "a", encoding="utf-8") as f:
        for eid in event_ids:
            f.write(eid + "\n")

def send_capi_batch(events):
    url = f"https://graph.facebook.com/v18.0/{PIXEL_ID}/events?access_token={ACCESS_TOKEN}"
    payload = {"data": events}
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            response = requests.post(url, json=payload, timeout=10)
            if response.status_code == 200:
                return True, response.text
            log(f"âŒ ç¬¬ {attempt} æ¬¡ä¸Šå‚³å¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {response.status_code}ï¼Œè¨Šæ¯: {response.text}")
        except Exception as e:
            log(f"âŒ ç¬¬ {attempt} æ¬¡ä¸Šå‚³éŒ¯èª¤ï¼š{e}")
        time.sleep(2)
    return False, None

def fix_encoding(text):
    try:
        return text.encode('latin1').decode('utf-8')
    except:
        return text

def is_valid_email(email):
    return re.match(r"[^@]+@[^@]+\.[^@]+", email)

def is_valid_phone(phone):
    return re.match(r"^09\d{8}$", phone)

def extract_age_to_birthdate(text):
    patterns = [
        r"(?:æˆ‘|æ»¿|æ‰å‰›æ»¿)?\s*(\d{1,2})\s*æ­²",
        r"æˆ‘\s*(\d{1,2})\b",
        r"\b(\d{1,2})æ­²",
    ]
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            try:
                age = int(match.group(1))
                if 18 <= age <= 80:
                    today = datetime.today()
                    year = today.year - age
                    month = random.randint(1, 12)
                    day = random.randint(1, 28)
                    return f"{year:04d}{month:02d}{day:02d}"
            except:
                continue
    return ""

def extract_birthdate(text):
    from_age = extract_age_to_birthdate(text)
    if from_age:
        return from_age
    match = re.search(r"(19|20)\d{2}[/-]?\d{1,2}[/-]?\d{1,2}", text)
    if match:
        digits = re.sub(r"[^\d]", "", match.group())
        return digits.zfill(8)
    year = random.randint(1980, 2005)
    month = random.randint(1, 12)
    day = random.randint(1, 28)
    return f"{year:04d}{month:02d}{day:02d}"

def extract_gender(text):
    text = text.lower()
    patterns_female = [r"æˆ‘(æ˜¯)?(å¥³ç”Ÿ|å¥³å­©|å°å§|å¥³æ€§)", r"(æˆ‘æ˜¯)?[0-9]{1,2}æ­²(çš„)?å¥³ç”Ÿ", r"æˆ‘è€å©†", r"æˆ‘å¥³å‹", r"æˆ‘å¥³æœ‹å‹"]
    patterns_male = [r"æˆ‘(æ˜¯)?(ç”·ç”Ÿ|ç”·å­©|å…ˆç”Ÿ|ç”·æ€§)", r"(æˆ‘æ˜¯)?[0-9]{1,2}æ­²(çš„)?ç”·ç”Ÿ", r"æˆ‘è€å…¬", r"æˆ‘ç”·å‹", r"æˆ‘ç”·æœ‹å‹"]
    for pattern in patterns_female:
        if re.search(pattern, text):
            return "f"
    for pattern in patterns_male:
        if re.search(pattern, text):
            return "m"
    if "å¥¹" in text:
        return "m"
    return "f"

def extract_zip(text):
    match = re.search(r"\b\d{3,5}\b", text)
    return match.group() if match else ""

def merge_profile(user_key, new_data):
    profile = user_profile_map.get(user_key, {})
    for key, value in new_data.items():
        if not profile.get(key) and value:
            profile[key] = value
    user_profile_map[user_key] = profile

def process_instagram_json_file(filepath, uploaded_event_ids):
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            data = json.load(f)
    except Exception as e:
        log(f"âš ï¸ ç„¡æ³•è™•ç†æª”æ¡ˆ {filepath}: {e}")
        return [], []

    if "messages" not in data:
        return [], []

    participants = data.get("participants", [])
    name = fix_encoding(participants[0].get("name", "")) if participants else ""
    username = fix_encoding(participants[0].get("username", "")) if participants else ""
    raw_email = fix_encoding(participants[0].get("email", ""))
    raw_phone = fix_encoding(participants[0].get("phone", ""))
    ip = participants[0].get("ip", "")

    email = raw_email if is_valid_email(raw_email) else ""
    phone = raw_phone if is_valid_phone(raw_phone) else ""
    user_key = username or name
    if user_key not in user_city_map:
        user_city_map[user_key] = random.choice(CITIES)
    city = user_city_map[user_key]

    messages = data["messages"]
    events, records = [], []

    for msg in messages:
        raw_text = msg.get("content", "")
        if isinstance(raw_text, list):
            raw_text = " ".join(str(t) for t in raw_text)
        elif not isinstance(raw_text, str):
            raw_text = str(raw_text)
        try:
            raw_text = raw_text.encode('latin1').decode('utf-8')
        except:
            pass

        sender = msg.get("sender_name") or msg.get("sender") or ""
        matched_keyword = next((kw for kw in KEYWORDS if kw in raw_text), None)
        if matched_keyword:
            event_time = int(time.time())
            event_id = generate_event_id(username or name, raw_text, event_time)
            if event_id in uploaded_event_ids:
                log(f"â­ï¸ å·²ä¸Šå‚³éçš„äº‹ä»¶ï¼ˆevent_id: {event_id}ï¼‰ï¼Œç•¥é")
                continue

            is_customer = sender not in MY_ACCOUNT_NAMES
            birthdate = extract_birthdate(raw_text) if is_customer else ""
            gender = extract_gender(raw_text) if is_customer else "f"
            price = random.choice(VALUE_CHOICES)
            zip_code = extract_zip(raw_text)

            new_profile = {
                "fn": name[:1] if name else "",
                "ln": name[1:] if name and len(name) > 1 else "",
                "ge": gender,
                "db": birthdate,
                "zp": zip_code,
                "em": email,
                "ph": phone,
                "ip": ip,
                "ct": city,
                "st": "taiwan",
                "country": "tw",
                "external_id": username or name
            }
            merge_profile(user_key, new_profile)

            profile = user_profile_map.get(user_key, {})
            user_data = {k: hash_data(profile.get(k, "")) for k in ["fn", "ln", "ge", "db", "zp", "ct", "st", "country", "em", "ph", "external_id"]}
            if profile.get("ip") and profile.get("ip").lower() not in ["null", "none"]:
                user_data["client_ip_address"] = profile["ip"]

            event = {
                "event_name": "Purchase",
                "event_time": event_time,
                "event_id": event_id,
                "user_data": user_data,
                "custom_data": {"currency": CURRENCY, "value": price}
            }
            events.append(event)
            records.append({
                "è³‡æ–™å¤¾": os.path.basename(os.path.dirname(filepath)),
                "æª”å": os.path.basename(filepath),
                "æˆäº¤": "âœ… æ˜¯",
                "å‘½ä¸­é—œéµå­—": matched_keyword,
                "å°è©±ç‰‡æ®µ": raw_text,
                "ç™¼è©±è€…": sender,
                "æ™‚é–“": event_time,
                "event_id": event_id,
                "å§“å": name,
                "å¸³è™Ÿ": username,
                "æ˜¯å¦ä¸Šå‚³æˆåŠŸ": ""
            })

    return events, records

def main():
    log("âš™ï¸ ç¨‹å¼å•Ÿå‹•å®Œæˆï¼Œé–‹å§‹è®€å–è³‡æ–™...")
    all_events, all_records = [], []
    uploaded_event_ids = load_uploaded_event_ids()

    for root, _, files in os.walk(BASE_FOLDER):
        for file in files:
            if file.endswith(".json") and not any(skip_kw in file.lower() for skip_kw in SKIP_KEYWORDS):
                filepath = os.path.join(root, file)
                events, records = process_instagram_json_file(filepath, uploaded_event_ids)
                all_events.extend(events)
                all_records.extend(records)

    if not all_events:
        log("âš ï¸ æ²’æœ‰åµæ¸¬åˆ°ä»»ä½•æˆäº¤äº‹ä»¶")
        return

    log(f"ğŸ“¦ å…±éœ€ä¸Šå‚³ {len(all_events)} ç­†äº‹ä»¶")
    newly_uploaded_ids = []

    for i in range(0, len(all_events), BATCH_SIZE):
        batch = all_events[i:i + BATCH_SIZE]
        success, _ = send_capi_batch(batch)
        for j in range(i, i + len(batch)):
            all_records[j]["æ˜¯å¦ä¸Šå‚³æˆåŠŸ"] = "âœ… æ˜¯" if success else "âŒ å¦"
            if success:
                newly_uploaded_ids.append(batch[j - i]["event_id"])
        log(f"{'âœ… æˆåŠŸ' if success else 'âŒ å¤±æ•—'} ä¸Šå‚³ç¬¬ {(i // BATCH_SIZE) + 1} æ‰¹")
        time.sleep(1)

    if newly_uploaded_ids:
        save_uploaded_event_ids(newly_uploaded_ids)
        log(f"âœ… å·²è¨˜éŒ„ {len(newly_uploaded_ids)} ç­† event_id")

    with open(OUTPUT_CSV, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=["è³‡æ–™å¤¾", "æª”å", "æˆäº¤", "å‘½ä¸­é—œéµå­—", "å°è©±ç‰‡æ®µ", "ç™¼è©±è€…", "æ™‚é–“", "event_id", "å§“å", "å¸³è™Ÿ", "æ˜¯å¦ä¸Šå‚³æˆåŠŸ"])
        writer.writeheader()
        writer.writerows(all_records)

    with open(PROFILE_MAP_PATH, "wb") as f:
        pickle.dump(user_profile_map, f)
    log(f"ğŸ“ çµ±ä¸€è³‡æ–™æª”å·²æ›´æ–°ï¼š{PROFILE_MAP_PATH}")
    log(f"ğŸ“„ æ‰€æœ‰äº‹ä»¶è¨˜éŒ„å·²å„²å­˜ï¼š{OUTPUT_CSV}")

if __name__ == "__main__":
    main()
