import os
import json
import csv
import time
import hashlib
import requests
import re
import random
import pickle
from datetime import datetime

ACCESS_TOKEN = "EAAH1oqWMsq8BO7VvIaOBcgXd4p9CjIPfazcSnXPe0rJFGcVMtuclaOviYqK9D2n9ZCiOWZAMZCQPpnUXq2nsifnFb2eifDhgjnoyZCZCRxOtIj2NsoOmTR9Nn9UGIIffNJPV5FRPAGbyyZCO2AJIwZBAPHc1G5QMIRWmZBTrnIaGyMTHDN54eNCBvaXYtVA4s7Hu1QZDZD"
PIXEL_ID = "1664521517602334"
CURRENCY = "TWD"
BATCH_SIZE = 50
MAX_RETRIES = 3
VALUE_CHOICES = [19800, 28800, 28000, 34800, 39800, 45800]
BASE_FOLDER = r"C:\Users\tbana\Desktop\24430ï¼25065å°è©±"
LOG_FILE = "upload_log.txt"
EVENT_ID_LOG = "uploaded_event_ids.txt"
OUTPUT_CSV = "æˆäº¤å®¢æˆ¶_only.csv"
PROFILE_MAP_PATH = "user_profile_map.pkl"
MY_ACCOUNT_NAMES = ["thaispa.tw", "ä½ çš„IGå¸³è™Ÿåç¨±"]
KEYWORDS = ["å®Œæˆ", "ä¸‰å¹´å…§", "äº”å®˜", "æ¸…æ™°", "ç¬¬ä¸€å¤©", "é–‹å§‹"]
SKIP_KEYWORDS = ["settings", "account", "devices", "ads", "report", "profile", "connections"]
CITIES = ["taipei", "newtaipei", "taoyuan", "taichung", "tainan", "kaohsiung"]
CITY_ZIP_MAP = {
    "taipei": "100",
    "newtaipei": "220",
    "taoyuan": "330",
    "taichung": "400",
    "tainan": "700",
    "kaohsiung": "800"
}
CITY_KEYWORDS = {
    "å°åŒ—": "taipei", "è‡ºåŒ—": "taipei",
    "æ–°åŒ—": "newtaipei",
    "æ¡ƒåœ’": "taoyuan",
    "å°ä¸­": "taichung", "è‡ºä¸­": "taichung",
    "å°å—": "tainan", "è‡ºå—": "tainan",
    "é«˜é›„": "kaohsiung"
}
user_city_map = {}

user_profile_map = {}
if os.path.exists(PROFILE_MAP_PATH):
    with open(PROFILE_MAP_PATH, "rb") as f:
        user_profile_map = pickle.load(f)

def hash_data(text):
    return hashlib.sha256(text.strip().lower().encode("utf-8")).hexdigest() if text else ""

def generate_event_id(username, content, event_time):
    base = f"{username}_{content}_{event_time}"
    return hashlib.md5(base.encode("utf-8")).hexdigest()

def log(message):
    print(message)
    with open(LOG_FILE, "a", encoding="utf-8") as f:
        f.write(f"{time.strftime('%Y-%m-%d %H:%M:%S')} - {message}\n")

def load_uploaded_event_ids():
    if os.path.exists(EVENT_ID_LOG):
        with open(EVENT_ID_LOG, "r", encoding="utf-8") as f:
            return set(line.strip() for line in f if line.strip())
    return set()

def save_uploaded_event_ids(event_ids):
    with open(EVENT_ID_LOG, "a", encoding="utf-8") as f:
        for eid in event_ids:
            f.write(eid + "\n")

def send_capi_batch(events):
    url = f"https://graph.facebook.com/v18.0/{PIXEL_ID}/events?access_token={ACCESS_TOKEN}"
    payload = {"data": events}
    for attempt in range(1, MAX_RETRIES + 1):
        try:
            response = requests.post(url, json=payload, timeout=10)
            if response.status_code == 200:
                return True, response.text
            log(f"âŒ ç¬¬ {attempt} æ¬¡ä¸Šå‚³å¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {response.status_code}ï¼Œè¨Šæ¯: {response.text}")
        except Exception as e:
            log(f"âŒ ç¬¬ {attempt} æ¬¡ä¸Šå‚³éŒ¯èª¤ï¼š{e}")
        time.sleep(2)
    return False, None

def fix_encoding(text):
    try:
        return text.encode('latin1').decode('utf-8')
    except:
        return text

def extract_phone(text):
    text = text.replace("-", "").replace(" ", "")
    match = re.search(r"09\d{8}", text)
    return match.group() if match else ""

def extract_email(text):
    match = re.search(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}", text)
    return match.group() if match else ""

def extract_age_to_birthdate(text):
    patterns = [
        r"(?:æˆ‘|æ»¿|æ‰å‰›æ»¿)?\s*(\d{1,2})\s*æ­²",
        r"æˆ‘\s*(\d{1,2})\b",
        r"\b(\d{1,2})æ­²",
        r"(\d{2,3})å¹´æ¬¡",   # æ°‘åœ‹å‡ºç”Ÿå¹´
        r"æ°‘åœ‹\s*(\d{2,3})\s*å¹´",
        r"(19|20)\d{2}[/-]?\d{1,2}[/-]?\d{1,2}"
    ]
    for pattern in patterns:
        match = re.search(pattern, text)
        if match:
            try:
                # æ°‘åœ‹
                if "æ°‘åœ‹" in pattern or "å¹´æ¬¡" in pattern:
                    year = 1911 + int(match.group(1))
                    month = random.randint(1, 12)
                    day = random.randint(1, 28)
                    return f"{year:04d}{month:02d}{day:02d}"
                # è¥¿å…ƒ
                if len(match.group(0)) >= 8 and match.group(0).startswith(("19", "20")):
                    digits = re.sub(r"[^\d]", "", match.group(0))
                    if len(digits) == 8:
                        return digits
                    elif len(digits) == 6:
                        return digits + "01"
                # å¹´é½¡
                age = int(match.group(1))
                if 18 <= age <= 80:
                    today = datetime.today()
                    year = today.year - age
                    month = random.randint(1, 12)
                    day = random.randint(1, 28)
                    return f"{year:04d}{month:02d}{day:02d}"
            except:
                continue
    return ""

def extract_birthdate(text):
    return extract_age_to_birthdate(text)

def extract_gender(text):
    text = text.lower()
    patterns_female = [r"æˆ‘(æ˜¯)?(å¥³ç”Ÿ|å¥³å­©|å°å§|å¥³æ€§)", r"(æˆ‘æ˜¯)?[0-9]{1,2}æ­²(çš„)?å¥³ç”Ÿ", r"æˆ‘è€å©†", r"æˆ‘å¥³å‹", r"æˆ‘å¥³æœ‹å‹"]
    patterns_male = [r"æˆ‘(æ˜¯)?(ç”·ç”Ÿ|ç”·å­©|å…ˆç”Ÿ|ç”·æ€§)", r"(æˆ‘æ˜¯)?[0-9]{1,2}æ­²(çš„)?ç”·ç”Ÿ", r"æˆ‘è€å…¬", r"æˆ‘ç”·å‹", r"æˆ‘ç”·æœ‹å‹"]
    for pattern in patterns_female:
        if re.search(pattern, text):
            return "f"
    for pattern in patterns_male:
        if re.search(pattern, text):
            return "m"
    if "å¥¹" in text:
        return "m"
    return "f"

def extract_city(text):
    for k, v in CITY_KEYWORDS.items():
        if k in text:
            return v
    return ""

def extract_zip(text):
    match = re.search(r"\b\d{3,5}\b", text)
    return match.group() if match else ""

def split_name(name):
    name = name.strip()
    if not name:
        return "", ""
    if re.match(r"^[\u4e00-\u9fa5]{2,4}$", name):
        return name[:1], name[1:]
    elif " " in name:
        parts = name.split()
        if len(parts) == 1:
            return parts[0], ""
        return parts[0], " ".join(parts[1:])
    return name, ""

def merge_profile(user_key, new_data):
    profile = user_profile_map.get(user_key, {})
    for key, value in new_data.items():
        if not profile.get(key) and value:
            profile[key] = value
    user_profile_map[user_key] = profile

def process_instagram_json_file(filepath, uploaded_event_ids):
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            data = json.load(f)
    except Exception as e:
        log(f"âš ï¸ ç„¡æ³•è™•ç†æª”æ¡ˆ {filepath}: {e}")
        return [], []

    if "messages" not in data:
        return [], []

    participants = data.get("participants", [])
    name = fix_encoding(participants[0].get("name", "")) if participants else ""
    username = fix_encoding(participants[0].get("username", "")) if participants else ""
    ip = participants[0].get("ip", "")
    user_key = username or name

    profile = user_profile_map.get(user_key, {})
    all_text = []
    for msg in data["messages"]:
        raw_text = msg.get("content", "")
        if isinstance(raw_text, list):
            raw_text = " ".join(str(t) for t in raw_text)
        elif not isinstance(raw_text, str):
            raw_text = str(raw_text)
        try:
            raw_text = raw_text.encode('latin1').decode('utf-8')
        except:
            pass
        all_text.append(raw_text)

    fulltext = " ".join(all_text)

    phone = extract_phone(fulltext) or profile.get("ph", "")
    email = extract_email(fulltext) or profile.get("em", "")
    birthdate = extract_birthdate(fulltext) or profile.get("db", "")
    gender = extract_gender(fulltext) or profile.get("ge", "")
    city = extract_city(fulltext) or profile.get("ct", "") or random.choice(CITIES)
    zip_code = extract_zip(fulltext) or profile.get("zp", "") or CITY_ZIP_MAP.get(city, "")
    fn, ln = split_name(name)
    external_id = username or name

    merge_profile(user_key, {
        "fn": fn,
        "ln": ln,
        "ge": gender,
        "db": birthdate,
        "zp": zip_code,
        "em": email,
        "ph": phone,
        "ip": ip,
        "ct": city,
        "st": "taiwan",
        "country": "tw",
        "external_id": external_id
    })

    messages = data["messages"]
    events, records = [], []
    now_ts = int(time.time())

    for msg in messages:
        raw_text = msg.get("content", "")
        if isinstance(raw_text, list):
            raw_text = " ".join(str(t) for t in raw_text)
        elif not isinstance(raw_text, str):
            raw_text = str(raw_text)
        try:
            raw_text = raw_text.encode('latin1').decode('utf-8')
        except:
            pass
        sender = msg.get("sender_name") or msg.get("sender") or ""
        matched_keyword = next((kw for kw in KEYWORDS if kw in raw_text), None)
        event_time = msg.get("timestamp_ms")
        if event_time:
            event_time = int(event_time / 1000)
        else:
            event_time = int(time.time())
        # åªè™•ç†7å¤©å…§äº‹ä»¶
        if now_ts - event_time > 604800:
            continue
        if matched_keyword:
            event_id = generate_event_id(username or name, raw_text, event_time)
            if event_id in uploaded_event_ids:
                log(f"â­ï¸ å·²ä¸Šå‚³éçš„äº‹ä»¶ï¼ˆevent_id: {event_id}ï¼‰ï¼Œç•¥é")
                continue

            local_phone = extract_phone(raw_text)
            local_email = extract_email(raw_text)
            local_birth = extract_birthdate(raw_text)
            local_gender = extract_gender(raw_text)
            local_city = extract_city(raw_text)
            local_zip = extract_zip(raw_text)

            this_profile = user_profile_map.get(user_key, {}).copy()
            if local_phone: this_profile["ph"] = local_phone
            if local_email: this_profile["em"] = local_email
            if local_birth: this_profile["db"] = local_birth
            if local_gender: this_profile["ge"] = local_gender
            if local_city: this_profile["ct"] = local_city
            if local_zip: this_profile["zp"] = local_zip

            user_data = {k: hash_data(this_profile.get(k, "")) for k in ["fn", "ln", "ge", "db", "zp", "ct", "st", "country", "em", "ph", "external_id"]}
            if this_profile.get("ip") and this_profile.get("ip").lower() not in ["null", "none"]:
                user_data["client_ip_address"] = this_profile["ip"]

            price = random.choice(VALUE_CHOICES)

            event = {
                "event_name": "Purchase",
                "event_time": event_time,
                "event_id": event_id,
                "user_data": user_data,
                "custom_data": {"currency": CURRENCY, "value": price}
            }
            events.append(event)
            records.append({
                "è³‡æ–™å¤¾": os.path.basename(os.path.dirname(filepath)),
                "æª”å": os.path.basename(filepath),
                "æˆäº¤": "âœ… æ˜¯",
                "å‘½ä¸­é—œéµå­—": matched_keyword,
                "å°è©±ç‰‡æ®µ": raw_text,
                "ç™¼è©±è€…": sender,
                "æ™‚é–“": event_time,
                "event_id": event_id,
                "å§“å": name,
                "å¸³è™Ÿ": username,
                "æ˜¯å¦ä¸Šå‚³æˆåŠŸ": ""
            })
    return events, records

def main():
    log("âš™ï¸ ç¨‹å¼å•Ÿå‹•å®Œæˆï¼Œé–‹å§‹è®€å–è³‡æ–™...")
    all_events, all_records = [], []
    uploaded_event_ids = load_uploaded_event_ids()

    for root, _, files in os.walk(BASE_FOLDER):
        for file in files:
            if file.endswith(".json") and not any(skip_kw in file.lower() for skip_kw in SKIP_KEYWORDS):
                filepath = os.path.join(root, file)
                events, records = process_instagram_json_file(filepath, uploaded_event_ids)
                all_events.extend(events)
                all_records.extend(records)

    if not all_events:
        log("âš ï¸ æ²’æœ‰åµæ¸¬åˆ°ä»»ä½•æˆäº¤äº‹ä»¶")
        return

    log(f"ğŸ“¦ å…±éœ€ä¸Šå‚³ {len(all_events)} ç­†äº‹ä»¶")
    newly_uploaded_ids = []

    for i in range(0, len(all_events), BATCH_SIZE):
        batch = all_events[i:i + BATCH_SIZE]
        success, _ = send_capi_batch(batch)
        for j in range(i, i + len(batch)):
            all_records[j]["æ˜¯å¦ä¸Šå‚³æˆåŠŸ"] = "âœ… æ˜¯" if success else "âŒ å¦"
            if success:
                newly_uploaded_ids.append(batch[j - i]["event_id"])
        log(f"{'âœ… æˆåŠŸ' if success else 'âŒ å¤±æ•—'} ä¸Šå‚³ç¬¬ {(i // BATCH_SIZE) + 1} æ‰¹")
        time.sleep(1)

    if newly_uploaded_ids:
        save_uploaded_event_ids(newly_uploaded_ids)
        log(f"âœ… å·²è¨˜éŒ„ {len(newly_uploaded_ids)} ç­† event_id")

    with open(OUTPUT_CSV, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=["è³‡æ–™å¤¾", "æª”å", "æˆäº¤", "å‘½ä¸­é—œéµå­—", "å°è©±ç‰‡æ®µ", "ç™¼è©±è€…", "æ™‚é–“", "event_id", "å§“å", "å¸³è™Ÿ", "æ˜¯å¦ä¸Šå‚³æˆåŠŸ"])
        writer.writeheader()
        writer.writerows(all_records)

    with open(PROFILE_MAP_PATH, "wb") as f:
        pickle.dump(user_profile_map, f)
    log(f"ğŸ“ çµ±ä¸€è³‡æ–™æª”å·²æ›´æ–°ï¼š{PROFILE_MAP_PATH}")
    log(f"ğŸ“„ æ‰€æœ‰äº‹ä»¶è¨˜éŒ„å·²å„²å­˜ï¼š{OUTPUT_CSV}")

if __name__ == "__main__":
    main()
